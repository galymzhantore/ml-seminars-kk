{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ccb980",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Оптимизация: градиентный спуск, метод Ньютона, SGD (+моментум)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d992446",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<style>\n",
    ".reveal .slides section {\n",
    "    height: 100% !important;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22c0b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## План\n",
    "1. Повторение: градиенты, матрицы Гессе, выпуклость\n",
    "2. Градиентный спуск (GD)\n",
    "3. Метод Ньютона\n",
    "4. Стохастический градиентный спуск (SGD) и моментум\n",
    "5. Демонстрации и сравнения\n",
    "6. Упражнения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c17f09",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def line_plot(x, y, xlabel, ylabel, title):\n",
    "    plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def overlay_plot(series_list, xlabel, ylabel, title):\n",
    "    plt.figure()\n",
    "    for x, y, label in series_list:\n",
    "        plt.plot(x, y, label=label)\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f51511",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Теория\n",
    "\n",
    "### Градиенты, гессианы, выпуклость\n",
    "- Градиент $ \\nabla f(x) $ указывает направление наибольшего возрастания; для спуска берём $ -\\nabla f(x) $.\n",
    "- Гессиан $ \\nabla^2 f(x) $ описывает локальную кривизну.\n",
    "- Сильная выпуклость с параметром $ \\mu>0 $ и $L$-липшицевый градиент задают скорости сходимости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ff7b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Методы оптимизации\n",
    "\n",
    "### Нормальное уравнение (аналитическое решение для линейной регрессии)\n",
    "Для задачи наименьших квадратов можно явно найти оптимальные веса:\n",
    "$$ w^* = (X^{\\top} X)^{-1} X^{\\top} y. $$\n",
    "На практике применяем `np.linalg.solve`, чтобы не строить обратную матрицу, но идея та же: решаем систему размера $d \\times d$ один раз.\n",
    "\n",
    "**Почему в современной ML это используют редко:**\n",
    "- Построение $X^{\\top} X$ и решение системы стоит $\\mathcal{O}(d^3)$ по времени и $\\mathcal{O}(d^2)$ по памяти — дорого при большом числе признаков $d$.\n",
    "- При сильной коррелированности признаков или $d > n$ матрица становится вырожденной/плохо обусловленной, и решение нестабильно без регуляризации.\n",
    "- Метод предполагает, что весь датасет помещается в память и не поддерживает инкрементальные обновления, поэтому плохо подходит для потоковых или огромных данных.\n",
    "- Он работает только для квадратичной ошибки; для логистической регрессии и других моделей закрытой формы нет.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae18dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Градиентный спуск (GD)\n",
    "Обновление:  \n",
    "$$\n",
    "x_{t+1} = x_t - \\alpha \\nabla f(x_t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee7ecfd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def run_gd_1d(grad, x0, alpha, iters, clip=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786903d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Формула для 1D градиентного спуска**\n",
    "$$ x_{t+1} = x_t - \\alpha f'(x_t). $$\n",
    "Здесь $\\alpha$ — шаг обучения, а $f'(x_t)$ — производная функции в точке $x_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe64ab0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Метод Ньютона\n",
    "Обновление учитывает кривизну:  \n",
    "$$\n",
    "x_{t+1} = x_t - \\bigl[\\nabla^2 f(x_t)\\bigr]^{-1} \\nabla f(x_t).\n",
    "$$\n",
    "Квадратичная сходимость рядом с минимумом, если гессиан $ \\succ 0 $ и начальная точка достаточно близка.\n",
    "\n",
    "Для квадратичной функции $ f(x)=\\tfrac12 x^\\top Q x + b^\\top x + c $ с $Q\\succ 0$ достаточно одного шага до минимума.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d8c2a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def run_newton_1d(grad, hess, x0, iters, damping=0.0):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b31e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Формула для шага метода Ньютона**\n",
    "$$ x_{t+1} = x_t - [ f''(x_t) ]^{-1} f'(x_t). $$\n",
    "Демпфирование масштабирующее шаг можно записать как $x_{t+1} = x_t - \\tfrac{1}{1+\\lambda} [f''(x_t)]^{-1} f'(x_t)$ при параметре $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a025d51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Стохастический градиентный спуск (SGD)\n",
    "Для эмпирического риска $ f(x)=\\tfrac1N \\sum_{i=1}^N \\ell(x; z_i) $ используем стохастическую оценку:  \n",
    "$$\n",
    "x_{t+1} = x_t - \\alpha_t \\,\\widehat{\\nabla} f(x_t), \\quad \\mathbb{E}[\\widehat{\\nabla} f]=\\nabla f.\n",
    "$$\n",
    "При уменьшающихся шагах $ \\alpha_t $ SGD сходится в выпуклых задачах; моментум снижает дисперсию и ускоряет движение по узким долинам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a8ed9f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def run_sgd_1d(grad, x0, alpha=0.1, iters=30, batch_grad_fn=None, clip=1e3):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c21c44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Формула для 1D SGD**\n",
    "$$ x_{t+1} = x_t - \\alpha \\widehat{f}'(x_t). $$\n",
    "Здесь $\\widehat{f}'(x_t)$ — стохастическая оценка производной, вычисленная по мини-батчу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cc93d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Демо 1: градиентный спуск в 1D — влияние шага\n",
    "Функции:\n",
    "- $f_1(x)=x^2$ с $\\nabla f_1=2x$\n",
    "- $f_2(x)=x^3$ с $\\nabla f_2=3x^2$\n",
    "\n",
    "Сравниваем два шага на выпуклой квадратичной функции, затем повторяем на кубике, чтобы увидеть влияние кривизны и седел.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82651c0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m xs_b \u001b[38;5;241m=\u001b[39m run_gd_1d(g1, x0, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, iters\u001b[38;5;241m=\u001b[39miters)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Строим зависимость положения от итерации\u001b[39;00m\n\u001b[1;32m     17\u001b[0m overlay_plot(\n\u001b[1;32m     18\u001b[0m     [\n\u001b[0;32m---> 19\u001b[0m         (np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxs_a\u001b[49m\u001b[43m)\u001b[49m), xs_a, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha=0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m         (np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(xs_b)), xs_b, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha=0.8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     ],\n\u001b[1;32m     22\u001b[0m     xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGD on f(x)=x^2: position\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Строим зависимость значения функции от итерации\u001b[39;00m\n\u001b[1;32m     26\u001b[0m fa \u001b[38;5;241m=\u001b[39m [f1(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs_a]\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Настраиваем квадратичную функцию потерь: определяем цель и градиент\n",
    "def f1(x): return x**2\n",
    "def g1(x): return 2*x\n",
    "\n",
    "# Кубическая функция для последующего сравнения\n",
    "def f2(x): return x**3\n",
    "def g2(x): return 3*(x**2)\n",
    "\n",
    "x0 = 3.0\n",
    "iters = 20\n",
    "\n",
    "# Отслеживаем траектории ГС для двух шагов на квадратичной функции\n",
    "xs_a = run_gd_1d(g1, x0, alpha=0.1, iters=iters)\n",
    "xs_b = run_gd_1d(g1, x0, alpha=0.8, iters=iters)\n",
    "\n",
    "# Строим зависимость положения от итерации\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(len(xs_a)), xs_a, \"alpha=0.1\"),\n",
    "        (np.arange(len(xs_b)), xs_b, \"alpha=0.8\"),\n",
    "    ],\n",
    "    xlabel=\"iteration\", ylabel=\"x\", title=\"GD on f(x)=x^2: position\"\n",
    ")\n",
    "\n",
    "# Строим зависимость значения функции от итерации\n",
    "fa = [f1(x) for x in xs_a]\n",
    "fb = [f1(x) for x in xs_b]\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(len(fa)), fa, \"alpha=0.1\"),\n",
    "        (np.arange(len(fb)), fb, \"alpha=0.8\"),\n",
    "    ],\n",
    "    xlabel=\"iteration\", ylabel=\"f(x)\", title=\"GD on f(x)=x^2: loss\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931c244",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Интерпретация запусков для $f(x)=x^2$**\n",
    "- `alpha=0.1` устойчиво тянет точку к нулю, поэтому положение и значение функции убывают гладко.\n",
    "- `alpha=0.8` движется быстрее, но перелетает минимум, из-за чего кривые колеблются, прежде чем стабилизироваться.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6048fea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Поведение на f(x)=x^3: градиент почти нулевой около 0 при x>=0\n",
    "xs_c = run_gd_1d(g2, x0=2.0, alpha=0.1, iters=30)\n",
    "xs_d = run_gd_1d(g2, x0=-2.0, alpha=0.1, iters=30)\n",
    "\n",
    "# Сравниваем, как старт по разные стороны седла меняет траекторию\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(len(xs_c)), xs_c, \"start=2.0\"),\n",
    "        (np.arange(len(xs_d)), xs_d, \"start=-2.0\"),\n",
    "    ],\n",
    "    xlabel=\"iteration\", ylabel=\"x\", title=\"GD on f(x)=x^3: position\"\n",
    ")\n",
    "\n",
    "# Отслеживаем значение цели, чтобы показать медленный прогресс в плоских областях\n",
    "fc = [f2(x) for x in xs_c]\n",
    "fd = [f2(x) for x in xs_d]\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(len(fc)), fc, \"start=2.0\"),\n",
    "        (np.arange(len(fd)), fd, \"start=-2.0\"),\n",
    "    ],\n",
    "    xlabel=\"iteration\", ylabel=\"f(x)\", title=\"GD on f(x)=x^3: loss\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8158f83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Интерпретация запусков для $f(x)=x^3$**\n",
    "- При старте `x0=2.0` траектория замирает около 0: градиент выравнивается, и прогресс сильно замедляется.\n",
    "- При старте `x0=-2.0` отрицательный наклон кубической функции уводит точку из седла гораздо быстрее.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c18726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Демо 2: метод Ньютона в 1D\n",
    "- Квадратичная функция: один шаг благодаря постоянному гессиану.\n",
    "- Невыпуклая квартичная функция: знак кривизны меняется; демпфирование стабилизирует шаги.\n",
    "\n",
    "Сначала решаем простую квадратичную задачу, чтобы показать скорость Ньютона, затем смотрим, как демпфирование спасает метод при отрицательном гессиане.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894693ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Квадратичная f(x) = (x-5)^2 => grad=2(x-5), hess=2\n",
    "def fq(x): return (x-5.0)**2\n",
    "def gq(x): return 2*(x-5.0)\n",
    "def hq(x): return 2.0\n",
    "\n",
    "# Один шаг Ньютона попадает точно в минимум\n",
    "xs_q = run_newton_1d(gq, hq, x0=20.0, iters=3)\n",
    "line_plot(np.arange(len(xs_q)), xs_q, \"iteration\", \"x\", \"Newton on quadratic: position\")\n",
    "\n",
    "# Невыпуклая f(x) = x^4 - 3x^2 + 2\n",
    "def fn(x): return x**4 - 3*(x**2) + 2.0\n",
    "def gn(x): return 4*(x**3) - 6*x\n",
    "def hn(x): return 12*(x**2) - 6\n",
    "\n",
    "# Сравниваем шаги Ньютона без демпфирования и с демпфированием\n",
    "xs_n1 = run_newton_1d(gn, hn, x0=0.5, iters=8, damping=0.0)\n",
    "xs_n2 = run_newton_1d(gn, hn, x0=0.5, iters=8, damping=1.0)  # демпфирование\n",
    "\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(len(xs_n1)), xs_n1, \"undamped\"),\n",
    "        (np.arange(len(xs_n2)), xs_n2, \"damped\"),\n",
    "    ],\n",
    "    \"iteration\",\n",
    "    \"x\",\n",
    "    \"Newton on non-convex: position\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e778fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Строим значения целевой функции для невыпуклого случая\n",
    "def seq_vals(xs, f):\n",
    "    return [f(x) for x in xs]\n",
    "\n",
    "# Вычисляем значения квартичной функции на каждой траектории, чтобы подчеркнуть эффект демпфирования\n",
    "ys1 = seq_vals(xs_n1, fn)\n",
    "ys2 = seq_vals(xs_n2, fn)\n",
    "\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(len(ys1)), ys1, \"undamped\"),\n",
    "        (np.arange(len(ys2)), ys2, \"damped\"),\n",
    "    ],\n",
    "    \"iteration\",\n",
    "    \"f(x)\",\n",
    "    \"Newton on non-convex: loss\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee85828",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Интерпретация графиков метода Ньютона**\n",
    "- На квадратичной функции Ньютон одним шагом попадает в минимум, поэтому график положения сразу падает к оптимуму.\n",
    "- Для невыпуклой квартичной функции траектория без демпфирования вылетает из области положительной кривизны и колеблется, тогда как демпфирование делает шаги осторожными и обеспечивает монотонное уменьшение потерь.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09572b08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Демо 4: бинарная логистическая регрессия — GD vs mini-batch SGD\n",
    "\n",
    "Датасет: синтетический 2D. Мы отслеживаем потерю по итерациям для двух оптимизаторов и смотрим на получившуюся границу.\n",
    "\n",
    "Шаги:\n",
    "1. Сгенерировать два гауссовых кластера и добавить столбец смещения.\n",
    "2. Обучить логистическую регрессию полным GD и мини-батчевым SGD.\n",
    "3. Сравнить кривые потерь и визуализировать границу решений мини-батчевого SGD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем разделимые гауссовы кластеры для бинарной задачи\n",
    "def make_logreg_data(n_samples=600):\n",
    "    \"\"\"Создаём два гауссовых кластера для бинарной классификации.\"\"\"\n",
    "    half = n_samples // 2\n",
    "    class_one = np.random.randn(half, 2) + np.array([1.5, 1.0])\n",
    "    class_zero = np.random.randn(half, 2) + np.array([-1.5, -1.0])\n",
    "    X = np.vstack([class_one, class_zero])\n",
    "    y = np.hstack([np.ones(half), np.zeros(half)])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883c098",
   "metadata": {},
   "source": [
    "**Формулы для вспомогательных функций**\n",
    "- Добавление смещения: $X_b = [\\,X\\; \\mathbf{1}\\,]$.\n",
    "- Сигмоида: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "- Потеря: $L(w) = -\\frac{1}{N} \\sum_{i=1}^{N} \\big[y_i \\log \\sigma(z_i) + (1-y_i) \\log (1-\\sigma(z_i))\\big] + \\tfrac{\\lambda}{2}\\lVert w \\rVert_2^2$.\n",
    "- Градиент: $\\nabla L(w) = \\frac{1}{N} X^\\top (\\sigma(X w) - y) + \\lambda w$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1241126",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Добавляем к признакам столбец со свободным членом\n",
    "def add_bias_column(X):\n",
    "    \"\"\"Добавляем к X столбец единиц для свободного члена.\"\"\"\n",
    "    pass\n",
    "\n",
    "def sigmoid(z):\n",
    "    pass\n",
    "\n",
    "def logistic_loss(w, X, y, reg=0.0):\n",
    "    \"\"\"Вычисляем логистическую функцию потерь.\"\"\"\n",
    "    pass\n",
    "\n",
    "def logistic_grad(w, X, y, reg=0.0):\n",
    "    \"\"\"Вычисляем градиент логистической функции потерь.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём данные один раз, чтобы все методы работали с одной выборкой\n",
    "X, y = make_logreg_data()\n",
    "X_with_bias = add_bias_column(X)  # исходный X нужен для отрисовки границы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9f45a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Базовый полно-батчевый градиентный спуск\n",
    "def gradient_descent_logreg(X, y, num_steps=200, step_size=0.5, reg=0.0):\n",
    "    \"\"\"Полно-батчевый градиентный спуск для логистической регрессии.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a90fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Формула для полно-батчевого GD по логистической потере**\n",
    "$$ w_{t+1} = w_t - \\eta \\left[ \\frac{1}{N} X^\\top (\\sigma(X w_t) - y) + \\lambda w_t \\right]. $$\n",
    "Здесь $\\sigma(z)=1/(1+e^{-z})$, $\\eta$ — шаг обучения, $N$ — число объектов, $\\lambda$ — коэффициент L2-регуляризации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_sgd_logreg(\n",
    "    X, y, num_steps=2000, step_size=0.05, batch_size=16, reg=0.0\n",
    "):\n",
    "    \"\"\"Мини-батчевый SGD без момента.\"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    loss_history = []\n",
    "    n_samples = X.shape[0]\n",
    "    for step in range(num_steps):\n",
    "        loss_history.append(logistic_loss(w, X, y, reg))\n",
    "        batch_idx = np.random.choice(n_samples, size=batch_size, replace=False)\n",
    "        grad = logistic_grad(w, X[batch_idx], y[batch_idx], reg)\n",
    "        w -= step_size * grad\n",
    "    return w, np.array(loss_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3416874",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Формула для mini-batch SGD**\n",
    "\\[ w_{t+1} = w_t - \\eta \\, g_t, \\quad g_t = \\frac{1}{B} X_B^\\top (\\sigma(X_B w_t) - y_B) + \\lambda w_t. \\]\n",
    "Здесь \\(B\\) — размер мини-батча, а \\(\\eta\\) — шаг обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa240749",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Общие гиперпараметры для всех оптимизаторов\n",
    "gd_config = dict(num_steps=200, step_size=0.5, reg=1e-3)\n",
    "sgd_config = dict(num_steps=2000, step_size=0.05, batch_size=16, reg=1e-3)\n",
    "\n",
    "# Обучаем два варианта на одних и тех же данных\n",
    "w_gd, loss_gd = gradient_descent_logreg(X_with_bias, y, **gd_config)\n",
    "w_sgd, loss_sgd = mini_batch_sgd_logreg(X_with_bias, y, **sgd_config)\n",
    "\n",
    "# Накладываем кривые потерь, чтобы сравнить скорость сходимости\n",
    "overlay_plot(\n",
    "    [\n",
    "        (np.arange(loss_gd.size), loss_gd, \"GD\"),\n",
    "        (np.arange(loss_sgd.size), loss_sgd, \"SGD (mini-batch 16)\"),\n",
    "    ],\n",
    "    \"iteration\",\n",
    "    \"loss\",\n",
    "    \"Logistic regression: GD vs mini-batch SGD\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865d539",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Интерпретация кривых потерь**\n",
    "- `GD` плавно уменьшает потерю и требует меньше итераций, потому что на каждом шаге использует весь датасет.\n",
    "- `SGD (mini-batch 16)` подёргивается вокруг оптимума: шумные оценки градиента вызывают колебания, хотя тренд направлен вниз.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e00674",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Визуализируем границу решений для мини-батчевого SGD\n",
    "def plot_boundary(X, y, w):\n",
    "    \"\"\"Строим границу решений логистической модели при вероятности 0.5.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], s=10)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], s=10)\n",
    "    xx = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n",
    "    yy = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 200)\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    grid = np.c_[XX.ravel(), YY.ravel(), np.ones(XX.size)]\n",
    "    Z = sigmoid(grid @ w).reshape(XX.shape)\n",
    "    plt.contour(XX, YY, Z, levels=[0.5])\n",
    "    plt.title(\"Decision boundary (p=0.5)\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "# Используем веса мини-батчевого SGD\n",
    "plot_boundary(X, y, w_sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824981a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Интерпретация границы решений**\n",
    "- Тёмный контур соответствует $p(y=1|x)=0.5$, поэтому точки выше отделяются как класс 1.\n",
    "- Прямая чисто разделяет два гауссовых облака, что означает, что логистическая модель выучила линейный разделитель.\n",
    "- Небольшое перекрытие в центре напоминает: шумные объекты могут остаться по неправильную сторону даже после сходимости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c011ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Упражнения\n",
    "1. **Поиск шага:** реализуйте обратный линейный поиск для GD на $x^3$ и сравните с фиксированными шагами.\n",
    "2. **Предобусловливание:** для 2D-квадратичной функции отмасштабируйте координаты и покажите улучшенный путь ГС.\n",
    "3. **Страховка Ньютона:** добавьте проверку, переключающуюся на ГС, когда гессиан $\\preceq 0$.\n",
    "4. **Расписание SGD:** попробуйте $\\alpha_t = \\alpha_0 / \\sqrt{t}$. Сравните шум и скорость.\n",
    "5. **Настройка момента:** изменяйте $\\beta \\in \\{0.5, 0.9, 0.99\\}$. Посчитайте итерации до потери $< 0.1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56babba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Приложение: ключевые формулы и скорости\n",
    "- ГС: $ x_{t+1} = x_t - \\alpha \\nabla f(x_t) $. Для сильно выпуклых задач скорость $ \\mathcal{O}((1-\\mu/L)^t) $.\n",
    "- Ньютон: $ x_{t+1}=x_t - [\\nabla^2 f(x_t)]^{-1}\\nabla f(x_t) $. Квадратичная сходимость возле оптимума.\n",
    "- SGD: $ x_{t+1}=x_t - \\alpha_t \\widehat{\\nabla} f(x_t) $. При убывающих шагах сходится в выпуклых задачах.\n",
    "- Моментум: $ v_{t+1}=\\beta v_t + \\nabla f(x_t) $, $ x_{t+1}=x_t-\\alpha v_{t+1} $.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kazakh_cefr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}