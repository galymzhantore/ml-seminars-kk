{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b708cad",
   "metadata": {},
   "source": [
    "\n",
    "# Autodiff in Practice: Forward-Mode, Reverse-Mode, VJPs\n",
    "\n",
    "This notebook is a hands-on companion to a lecture on automatic differentiation.  \n",
    "You will:\n",
    "- Build intuition for forward- and reverse-mode AD.\n",
    "- Work through small symbolic chains.\n",
    "- Implement backpropagation for a tiny linear–ReLU network in NumPy.\n",
    "- Verify gradients via finite differences.\n",
    "- Practice deriving Jacobian-vector products (JVPs) and vector-Jacobian products (VJPs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "def seed_all(seed=0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_all(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce109104",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1 — Forward-mode warm-up\n",
    "\n",
    "We track both values and directional derivatives as we **move forward**.\n",
    "\n",
    "Chain:\n",
    "\\begin{align}\n",
    "h_1 &= 2x,\\\\\n",
    "h_2 &= h_1^2,\\\\\n",
    "y &= \\sin(h_2).\n",
    "\\end{align}\n",
    "\n",
    "Forward-mode rules:\n",
    "\\begin{align}\n",
    "\\dot{h}_1 &= ?,\\\\\n",
    "\\dot{h}_2 &= ?,\\\\\n",
    "\\dot{y} &= ?.\n",
    "\\end{align}\n",
    "\n",
    "Run a quick numeric check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241048e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_chain(x):\n",
    "    # values\n",
    "    h1 = 2.0 * x\n",
    "    h2 = h1**2\n",
    "    y  = np.sin(h2)\n",
    "    # forward-mode directional derivatives w.r.t. x\n",
    "    dh1 = None\n",
    "    dh2 = None\n",
    "    dy  = None\n",
    "    return (h1,h2,y), (dh1,dh2,dy)\n",
    "\n",
    "# test\n",
    "x = 0.3\n",
    "(vals, ders) = forward_chain(x)\n",
    "vals, ders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c3b5e",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2 — Reverse-mode on the same chain\n",
    "\n",
    "Reverse-mode starts at the scalar output and **propagates adjoints backward**:\n",
    "\\begin{align}\n",
    "\\bar{y} &= 1,\\\\\n",
    "\\bar{h}_2 &= ?,\\\\\n",
    "\\bar{h}_1 &= ?,\\\\\n",
    "\\bar{x}  &= ?.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04150880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reverse_chain(x):\n",
    "    # forward pass: cache values\n",
    "    h1 = 2.0 * x\n",
    "    h2 = h1**2\n",
    "    y  = np.sin(h2)\n",
    "    # backward pass: adjoints\n",
    "    y_bar  = 1.0\n",
    "    h2_bar = None\n",
    "    h1_bar = None\n",
    "    x_bar  = None\n",
    "    return (h1,h2,y), (x_bar,h1_bar,h2_bar,y_bar)\n",
    "\n",
    "# test\n",
    "reverse_chain(0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6013237",
   "metadata": {},
   "source": [
    "\n",
    "### Check equivalence\n",
    "\n",
    "$\\dot{y} = \\frac{dy}{dx}$ from forward-mode should equal $\\bar{x} = \\frac{\\partial y}{\\partial x}$ from reverse-mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d08a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = 0.3\n",
    "(_, (_,_,dy)) = forward_chain(x)\n",
    "(_, (x_bar,_,_,_)) = reverse_chain(x)\n",
    "print(\"Forward-mode dy/dx:\", dy)\n",
    "print(\"Reverse-mode d y / d x:\", x_bar)\n",
    "assert np.allclose(dy, x_bar, atol=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670776ae",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3 — VJPs and JVPs for a linear layer\n",
    "\n",
    "For $f(x,W) = Wx$:\n",
    "- JVP w.r.t. $x$ with direction $u$: $\\mathrm{jvp}_x(f,u) = W u$.\n",
    "- VJP w.r.t. $x$ with vector $v$: $\\mathrm{vjp}_x(f,v) = W^\\top v$.\n",
    "- VJP w.r.t. $W$: $\\mathrm{vjp}_W(f,v) = v\\,x^\\top$.\n",
    "\n",
    "We will code the reverse-mode gradients directly for training a tiny network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d943a53",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4 — Tiny linear–ReLU network in NumPy\n",
    "\n",
    "Model:\n",
    "\\begin{align}\n",
    "h &= \\mathrm{ReLU}(W_1 x), \\\\\n",
    "y &= W_2 h, \\\\\n",
    "L &= \\|y - t\\|^2.\n",
    "\\end{align}\n",
    "\n",
    "Backprop:\n",
    "\\begin{align}\n",
    "\\bar{y} &= 2(y - t),\\\\\n",
    "\\bar{W}_2 &= \\bar{y}\\,h^\\top, \\quad \\bar{h} = W_2^\\top \\bar{y},\\\\\n",
    "\\bar{z} &= \\mathbf{1}_{z>0} \\odot \\bar{h}, \\quad z=W_1 x,\\\\\n",
    "\\bar{W}_1 &= \\bar{z}\\,x^\\top, \\quad \\bar{x} = W_1^\\top \\bar{z}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "def relu_grad(z):\n",
    "    return (z > 0).astype(z.dtype)\n",
    "\n",
    "def forward_linear_relu(W1, W2, x, t):\n",
    "    # forward\n",
    "    z = W1 @ x\n",
    "    h = relu(z)\n",
    "    y = W2 @ h\n",
    "    L = np.sum((y - t)**2)\n",
    "    cache = {\"x\":x, \"z\":z, \"h\":h, \"y\":y}\n",
    "    return L, cache\n",
    "\n",
    "def backward_linear_relu(W1, W2, cache, t):\n",
    "    # unpack\n",
    "    x, z, h, y = cache[\"x\"], cache[\"z\"], cache[\"h\"], cache[\"y\"]\n",
    "    # upstream from loss\n",
    "    y_bar = 2.0*(y - t)\n",
    "    # grads W2, h\n",
    "    dW2 = np.outer(y_bar, h)\n",
    "    h_bar = W2.T @ y_bar\n",
    "    # through ReLU\n",
    "    z_bar = relu_grad(z) * h_bar\n",
    "    dW1 = np.outer(z_bar, x)\n",
    "    x_bar = W1.T @ z_bar\n",
    "    return {\"dW1\": dW1, \"dW2\": dW2, \"dx\": x_bar}\n",
    "\n",
    "# sanity check\n",
    "seed_all(0)\n",
    "W1 = np.random.randn(8, 5)*0.1\n",
    "W2 = np.random.randn(3, 8)*0.1\n",
    "x  = np.random.randn(5)\n",
    "t  = np.random.randn(3)\n",
    "\n",
    "L, cache = forward_linear_relu(W1, W2, x, t)\n",
    "grads = backward_linear_relu(W1, W2, cache, t)\n",
    "L, grads[\"dW1\"].shape, grads[\"dW2\"].shape, grads[\"dx\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff1509",
   "metadata": {},
   "source": [
    "\n",
    "### Finite-difference gradient check\n",
    "\n",
    "We verify $\\nabla_{W_1} L$ and $\\nabla_{W_2} L$ numerically:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_{ij}} \\approx \\frac{L(\\theta_{ij}+\\epsilon) - L(\\theta_{ij}-\\epsilon)}{2\\epsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finite_diff_grad_W(W1, W2, x, t, which=\"W1\", eps=1e-5, num_checks=10, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = []\n",
    "    if which == \"W1\":\n",
    "        H, D = W1.shape\n",
    "        for _ in range(num_checks):\n",
    "            i = rng.integers(0, H)\n",
    "            j = rng.integers(0, D)\n",
    "            idxs.append((i,j))\n",
    "        analytic = backward_linear_relu(W1, W2, forward_linear_relu(W1, W2, x, t)[1], t)[\"dW1\"]\n",
    "    else:\n",
    "        O, H = W2.shape\n",
    "        for _ in range(num_checks):\n",
    "            i = rng.integers(0, O)\n",
    "            j = rng.integers(0, H)\n",
    "            idxs.append((i,j))\n",
    "        analytic = backward_linear_relu(W1, W2, forward_linear_relu(W1, W2, x, t)[1], t)[\"dW2\"]\n",
    "    max_err = 0.0\n",
    "    for (i,j) in idxs:\n",
    "        if which == \"W1\":\n",
    "            Wp = W1.copy(); Wm = W1.copy()\n",
    "            Wp[i,j] += eps; Wm[i,j] -= eps\n",
    "            Lp, _ = forward_linear_relu(Wp, W2, x, t)\n",
    "            Lm, _ = forward_linear_relu(Wm, W2, x, t)\n",
    "        else:\n",
    "            Wp = W2.copy(); Wm = W2.copy()\n",
    "            Wp[i,j] += eps; Wm[i,j] -= eps\n",
    "            Lp, _ = forward_linear_relu(W1, Wp, x, t)\n",
    "            Lm, _ = forward_linear_relu(W1, Wm, x, t)\n",
    "        num = (Lp - Lm) / (2*eps)\n",
    "        ana = analytic[i,j]\n",
    "        err = abs(num - ana)\n",
    "        max_err = max(max_err, err)\n",
    "        print(f\"Index {(i,j)}: analytic={ana:.6f}, numeric={num:.6f}, abs_err={err:.3e}\")\n",
    "    print(\"Max abs error:\", max_err)\n",
    "\n",
    "# run checks\n",
    "seed_all(1)\n",
    "W1 = np.random.randn(8, 5)*0.1\n",
    "W2 = np.random.randn(3, 8)*0.1\n",
    "x  = np.random.randn(5)\n",
    "t  = np.random.randn(3)\n",
    "print(\"Check dW1:\")\n",
    "finite_diff_grad_W(W1, W2, x, t, which=\"W1\", num_checks=8)\n",
    "print(\"\\nCheck dW2:\")\n",
    "finite_diff_grad_W(W1, W2, x, t, which=\"W2\", num_checks=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582bb07",
   "metadata": {},
   "source": [
    "\n",
    "## Part 5 — Mini-batch version\n",
    "\n",
    "For a batch $\\{(x^{(b)}, t^{(b)})\\}_{b=1}^B$, we sum outer products:\n",
    "$\\nabla_{W_2} L = \\sum_b \\bar{y}^{(b)} {h^{(b)}}^\\top$, $\\nabla_{W_1} L = \\sum_b \\bar{z}^{(b)} {x^{(b)}}^\\top$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201404f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_batch(W1, W2, X, T):\n",
    "    # X: (D, B), T: (O, B)\n",
    "    Z = W1 @ X\n",
    "    H = np.maximum(0.0, Z)\n",
    "    Y = W2 @ H\n",
    "    L = np.sum((Y - T)**2)\n",
    "    cache = {\"X\":X, \"Z\":Z, \"H\":H, \"Y\":Y}\n",
    "    return L, cache\n",
    "\n",
    "def backward_batch(W1, W2, cache, T):\n",
    "    X, Z, H, Y = cache[\"X\"], cache[\"Z\"], cache[\"H\"], cache[\"Y\"]\n",
    "    # upstream\n",
    "    Y_bar = 2.0*(Y - T)\n",
    "    dW2 = Y_bar @ H.T            # sum over batch is implicit via matrix product\n",
    "    H_bar = W2.T @ Y_bar\n",
    "    Z_bar = (Z > 0).astype(Z.dtype) * H_bar\n",
    "    dW1 = Z_bar @ X.T\n",
    "    dX  = W1.T @ Z_bar\n",
    "    return {\"dW1\": dW1, \"dW2\": dW2, \"dX\": dX}\n",
    "\n",
    "# test\n",
    "seed_all(2)\n",
    "D, H, O, B = 5, 8, 3, 4\n",
    "W1 = np.random.randn(H, D)*0.1\n",
    "W2 = np.random.randn(O, H)*0.1\n",
    "X  = np.random.randn(D, B)\n",
    "T  = np.random.randn(O, B)\n",
    "L, cache = forward_batch(W1, W2, X, T)\n",
    "grads = backward_batch(W1, W2, cache, T)\n",
    "L, grads[\"dW1\"].shape, grads[\"dW2\"].shape, grads[\"dX\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19004a7",
   "metadata": {},
   "source": [
    "\n",
    "## Practice — Derivations\n",
    "\n",
    "1. Forward-mode practice: For the chain\n",
    "   $$ u = 3x+1,\\quad v = e^{u},\\quad y = \\tanh(v), $$\n",
    "   derive $\\dot{y} = \\frac{dy}{dx}$ using forward-mode rules. Then check numerically.\n",
    "\n",
    "2. Reverse-mode practice: For the same chain, derive adjoints\n",
    "   $\\bar{y},\\bar{v},\\bar{u},\\bar{x}$.\n",
    "\n",
    "3. VJP practice: For $f(x,W)=Wx+b$ with bias $b$,\n",
    "   write $\\mathrm{vjp}_x(f,v)$, $\\mathrm{vjp}_W(f,v)$, and $\\mathrm{vjp}_b(f,v)$.\n",
    "\n",
    "4. Activation practice: For $\\phi(z)=\\mathrm{ReLU}(z)$, show that the VJP is\n",
    "   $\\bar{z} = \\mathbf{1}_{z>0} \\odot \\bar{y}$.\n",
    "\n",
    "Use the empty cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3710528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work: forward-mode practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work: reverse-mode practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work: VJP practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80870213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work: activation practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97fc85",
   "metadata": {},
   "source": [
    "\n",
    "## Part 6 — Memory vs Compute: Gradient Checkpointing (concept)\n",
    "\n",
    "Idea: store only selected activations during forward. In backward, if an activation is missing, recompute a short forward from the nearest stored checkpoint. This reduces memory at the cost of a small compute overhead.\n",
    "\n",
    "Demo sketch (not a full autodiff engine): we simulate storing only every k-th activation and \"recompute\" intermediate ones during backward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ddbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def toy_forward_with_checkpoints(xs, k=2):\n",
    "    # xs: list of scalars; build cumulative sum to simulate \"layers\"\n",
    "    acts = []\n",
    "    checkpoints = {}\n",
    "    s = 0.0\n",
    "    for i, x in enumerate(xs):\n",
    "        s = s + x*x  # pretend \"layer i\"\n",
    "        acts.append(s)\n",
    "        if i % k == 0:\n",
    "            checkpoints[i] = s  # store every k-th activation\n",
    "    return acts, checkpoints\n",
    "\n",
    "def toy_backward_recompute(xs, acts, checkpoints, k=2):\n",
    "    # need derivatives of last output wrt each x\n",
    "    # y = sum_{i} (x_i^2) composed cumulatively; derivative is simple,\n",
    "    # but we pretend we must \"recompute\" missing activations.\n",
    "    grads = np.zeros_like(xs, dtype=float)\n",
    "    upstream = 1.0  # d y / d last act\n",
    "    # walk backward; if act not stored, recompute from nearest checkpoint\n",
    "    for i in reversed(range(len(xs))):\n",
    "        if i not in checkpoints:\n",
    "            # recompute from nearest earlier checkpoint\n",
    "            # since layer function here is simple, we just recompute s locally\n",
    "            s = 0.0\n",
    "            start = (i // k)*k\n",
    "            s = checkpoints.get(start, 0.0)\n",
    "            for j in range(start+1, i+1):\n",
    "                s = s + xs[j]*xs[j]\n",
    "        else:\n",
    "            s = checkpoints[i]\n",
    "        # local derivative wrt x_i is 2*x_i\n",
    "        grads[i] = upstream * 2.0*xs[i]\n",
    "        # upstream for previous is unchanged in this toy\n",
    "    return grads\n",
    "\n",
    "xs = np.array([0.5, -1.2, 0.3, 0.7, -0.9])\n",
    "acts, ckpts = toy_forward_with_checkpoints(xs, k=2)\n",
    "grads = toy_backward_recompute(xs, acts, ckpts, k=2)\n",
    "print(\"checkpoints:\", ckpts)\n",
    "print(\"toy grads:\", grads, \"(true should be 2*x):\", 2*xs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da513d4",
   "metadata": {},
   "source": [
    "\n",
    "## Part 7 — Finite-difference helper for any scalar function\n",
    "\n",
    "Utility for validating a custom backward implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ba9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finite_diff_grad(fn, theta, eps=1e-5):\n",
    "    # theta is 1D array\n",
    "    g = np.zeros_like(theta)\n",
    "    for i in range(theta.size):\n",
    "        th_p = theta.copy(); th_m = theta.copy()\n",
    "        th_p[i] += eps; th_m[i] -= eps\n",
    "        g[i] = (fn(th_p) - fn(th_m)) / (2*eps)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a9883",
   "metadata": {},
   "source": [
    "\n",
    "## Practice — Build your own composite and verify\n",
    "\n",
    "1. Define a scalar function `fn(theta)` of your design (e.g., tiny MLP with ReLU).\n",
    "2. Write a manual backward for it.\n",
    "3. Compare analytical gradients to `finite_diff_grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ffdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example scaffold (replace with your own)\n",
    "def fn_example(theta):\n",
    "    # split theta into small pieces\n",
    "    W = theta[:4].reshape(2,2)\n",
    "    x = theta[4:6]\n",
    "    t = theta[6:8]\n",
    "    h = np.maximum(0.0, W @ x)\n",
    "    y = h  # identity\n",
    "    L = np.sum((y - t)**2)\n",
    "    return L\n",
    "\n",
    "theta = np.random.randn(8)*0.1\n",
    "g_num = finite_diff_grad(fn_example, theta)\n",
    "print(\"Numeric grad shape:\", g_num.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8177c",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- Forward-mode carries directional derivatives with values.\n",
    "- Reverse-mode backpropagates a single scalar loss to get all parameter gradients.\n",
    "- VJPs power reverse-mode; linear layer and ReLU VJP rules are simple and compose well.\n",
    "- Use finite differences to sanity-check gradients of small models.\n",
    "- Gradient checkpointing trades compute for memory in deep nets.\n",
    "\n",
    "You're done. Now extend any section with your own experiments.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
